#!/usr/bin/env python3
"""
Combine parquet files generated by the patent similarity MPI computation.
This script can handle large datasets by using PyArrow's efficient methods
for reading and writing parquet files without loading everything into memory.
Added error handling to skip corrupted files.
"""

import os
import glob
import argparse
import time
import logging
from pathlib import Path
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import numpy as np

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("combine_parquet.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger()

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Combine similarity parquet files.')
    parser.add_argument('--input-dir', type=str, default=None,
                        help='Directory containing the similarity parquet files')
    parser.add_argument('--output-file', type=str, default=None,
                        help='Path to the combined output file')
    parser.add_argument('--threshold', type=float, default=None,
                        help='Minimum similarity threshold to include (e.g., 0.7)')
    parser.add_argument('--chunk-size', type=int, default=1000000,
                        help='Number of rows to process at once')
    parser.add_argument('--top-n', type=int, default=None,
                        help='Keep only top N similar pairs per patent')
    parser.add_argument('--resume-from', type=int, default=0,
                        help='Resume processing from this file index')
    return parser.parse_args()

def get_parquet_files(input_dir):
    """Get all parquet files in the input directory."""
    # Match files like "similarities_rank0_part0.parquet"
    pattern = os.path.join(input_dir, "similarities_rank*_part*.parquet")
    files = glob.glob(pattern)
    
    # If no files found with the rank pattern, try simpler pattern
    if not files:
        pattern = os.path.join(input_dir, "similarities_part*.parquet")
        files = glob.glob(pattern)
    
    return sorted(files)  # Sort files to ensure consistent processing order

def get_parquet_schema(files):
    """Get the schema from the first valid parquet file."""
    for file in files:
        try:
            schema = pq.read_schema(file)
            return schema
        except Exception as e:
            logger.warning(f"Could not read schema from {file}: {e}")
    
    # If no valid file found, create a default schema
    logger.warning("No valid file found to extract schema, using default schema")
    return pa.schema([
        ('patent_id_1', pa.string()),
        ('patent_id_2', pa.string()),
        ('similarity', pa.float32())
    ])

def count_total_rows(files, chunk_size=1000000):
    """Count total number of rows across all parquet files."""
    total_rows = 0
    valid_files = []
    corrupted_files = []
    
    logger.info(f"Counting rows in {len(files)} files...")
    
    for i, file in enumerate(files):
        if i % 10 == 0:
            logger.info(f"Counting file {i+1}/{len(files)}")
        try:
            metadata = pq.read_metadata(file)
            total_rows += metadata.num_rows
            valid_files.append(file)
        except Exception as e:
            logger.warning(f"Error reading metadata from file {file}: {e}")
            corrupted_files.append(file)
    
    logger.info(f"Found {len(valid_files)} valid files with {total_rows:,} total rows")
    logger.info(f"Found {len(corrupted_files)} corrupted files")
    
    # Write list of corrupted files to a log file
    if corrupted_files:
        with open("corrupted_files.log", "w") as f:
            for file in corrupted_files:
                f.write(f"{file}\n")
        logger.info(f"List of corrupted files written to corrupted_files.log")
    
    return total_rows, valid_files

def combine_files_basic(files, output_file, threshold=None, chunk_size=1000000, resume_from=0):
    """Combine parquet files into a single file with optional similarity threshold."""
    if not files:
        logger.warning("No files found to combine.")
        return
    
    # Count rows and get valid files
    total_rows, valid_files = count_total_rows(files, chunk_size)
    if not valid_files:
        logger.error("No valid files to process. Exiting.")
        return
    
    # Get schema from valid files
    schema = get_parquet_schema(valid_files)
    
    # Create output directory if needed
    os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
    
    # Check if we're resuming
    append_mode = resume_from > 0
    
    # Slice files if resuming
    if resume_from > 0:
        logger.info(f"Resuming from file index {resume_from}")
        valid_files = valid_files[resume_from:]
    
    # Create output file writer - handle append mode correctly
    if append_mode:
        # Check if output file exists
        if not os.path.exists(output_file):
            logger.warning(f"Output file {output_file} does not exist. Starting from scratch.")
            append_mode = False
    
    # Create writer without using the 'append' parameter
    if append_mode:
        # For append mode, we need to read existing file and create a new one
        logger.info(f"Appending to existing file {output_file}")
        temp_file = output_file + ".temp"
        writer = pq.ParquetWriter(temp_file, schema)
        # We'll combine the files at the end
    else:
        writer = pq.ParquetWriter(output_file, schema)
    
    # Process each file
    processed_rows = 0
    processed_files = 0
    start_time = time.time()
    last_update_time = start_time
    checkpoint_interval = 100  # Save checkpoint after processing this many files
    
    for file_idx, file in enumerate(valid_files):
        # Save the current progress periodically
        if file_idx > 0 and file_idx % checkpoint_interval == 0:
            with open("combine_checkpoint.txt", "w") as f:
                f.write(f"{file_idx + resume_from}\n")
            logger.info(f"Checkpoint saved at file index {file_idx + resume_from}")
        
        # Show progress every 10 files
        if file_idx % 10 == 0:
            logger.info(f"Processing file {file_idx+1}/{len(valid_files)} ({file_idx + resume_from + 1} overall): {file}")
        
        try:
            # Read and process file in chunks
            reader = pq.ParquetFile(file)
            file_rows = 0
            
            for i in range(reader.num_row_groups):
                try:
                    table = reader.read_row_group(i)
                    df = table.to_pandas()
                    
                    # Apply similarity threshold if specified
                    if threshold is not None:
                        df = df[df['similarity'] >= threshold]
                    
                    if not df.empty:
                        # Ensure patent IDs are strings
                        df['patent_id_1'] = df['patent_id_1'].astype(str)
                        df['patent_id_2'] = df['patent_id_2'].astype(str)
                        
                        # Write chunk to output
                        chunk_table = pa.Table.from_pandas(df, schema=schema)
                        writer.write_table(chunk_table)
                        file_rows += len(df)
                
                except Exception as e:
                    logger.warning(f"Error processing row group {i} in file {file}: {e}")
            
            processed_rows += file_rows
            processed_files += 1
            
        except Exception as e:
            logger.warning(f"Error processing file {file}: {e}")
        
        # Show progress every 30 seconds
        current_time = time.time()
        if current_time - last_update_time > 30:
            elapsed = current_time - start_time
            progress = file_idx / len(valid_files) if valid_files else 0
            logger.info(f"Progress: {file_idx+1}/{len(valid_files)} files ({progress:.1%}), "
                      f"Rows: {processed_rows:,}, "
                      f"Elapsed: {elapsed:.1f}s, "
                      f"ETA: {elapsed/progress-elapsed:.1f}s" if progress > 0 else "Calculating...")
            last_update_time = current_time
    
    writer.close()
    
    # If we were appending, now we need to combine the original file with our temp file
    if append_mode:
        logger.info("Merging original file with new data...")
        try:
            # Read both files
            original_table = pq.read_table(output_file)
            new_table = pq.read_table(temp_file)
            
            # Combine them
            combined_table = pa.concat_tables([original_table, new_table])
            
            # Write back to original file
            pq.write_table(combined_table, output_file)
            
            # Clean up temp file
            os.remove(temp_file)
            logger.info("Files successfully merged")
        except Exception as e:
            logger.error(f"Error merging files: {e}")
            logger.info(f"New data is available in {temp_file}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    logger.info("Combination completed!")
    logger.info(f"Total files processed: {processed_files:,}/{len(valid_files):,}")
    logger.info(f"Total rows processed: {processed_rows:,}")
    logger.info(f"Output file: {output_file}")
    logger.info(f"Total processing time: {total_time:.2f} seconds")
    
    # Get file size
    output_size_bytes = os.path.getsize(output_file)
    output_size_gb = output_size_bytes / (1024**3)
    logger.info(f"Output file size: {output_size_gb:.2f} GB")
    
    # Clean up checkpoint if successful
    if os.path.exists("combine_checkpoint.txt"):
        os.remove("combine_checkpoint.txt")

def combine_files_with_top_n(files, output_file, top_n, threshold=None, chunk_size=1000000, resume_from=0):
    """Combine files and keep only top N similar pairs for each patent ID."""
    if not files:
        logger.warning("No files found to combine.")
        return
    
    # Count rows and get valid files
    _, valid_files = count_total_rows(files, chunk_size)
    if not valid_files:
        logger.error("No valid files to process. Exiting.")
        return
    
    # Slice files if resuming
    if resume_from > 0:
        logger.info(f"Resuming from file index {resume_from}")
        valid_files = valid_files[resume_from:]
    
    logger.info(f"Combining files with top {top_n} similar patents per patent ID...")
    
    # Check if we're resuming and should load existing data
    append_mode = resume_from > 0
    existing_data = {}
    
    if append_mode and os.path.exists(output_file):
        logger.info(f"Loading existing data from {output_file} for appending...")
        try:
            # Load existing data to merge with new results
            existing_df = pq.read_table(output_file).to_pandas()
            for patent_id, group in existing_df.groupby('patent_id_1'):
                existing_data[patent_id] = group.sort_values('similarity', ascending=False).head(top_n)
            logger.info(f"Loaded {len(existing_data)} existing patent IDs")
        except Exception as e:
            logger.warning(f"Error loading existing data: {e}")
            append_mode = False
    
    # Process in batches to manage memory usage
    all_data = existing_data  # Start with existing data if appending
    
    start_time = time.time()
    last_update_time = start_time
    checkpoint_interval = 100  # Save checkpoint after processing this many files
    temp_files = []  # Track temporary files
    
    for file_idx, file in enumerate(valid_files):
        # Save the current progress periodically
        if file_idx > 0 and file_idx % checkpoint_interval == 0:
            # Checkpoint progress
            with open("combine_checkpoint.txt", "w") as f:
                f.write(f"{file_idx + resume_from}\n")
            logger.info(f"Checkpoint saved at file index {file_idx + resume_from}")
            
            # Optionally save intermediate results
            if len(all_data) > 1000000:  # If we have accumulated a lot of data
                logger.info(f"Writing intermediate results to disk...")
                temp_output = f"{output_file}.temp.{file_idx}"
                _write_top_n_results(all_data, temp_output)
                temp_files.append(temp_output)
                logger.info(f"Intermediate results written to {temp_output}")
                # Clear memory
                all_data = {}
        
        if file_idx % 10 == 0:
            logger.info(f"Processing file {file_idx+1}/{len(valid_files)} ({file_idx + resume_from + 1} overall): {os.path.basename(file)}")
        
        try:
            # Read the file
            df = pq.read_table(file).to_pandas()
            
            # Apply similarity threshold if specified
            if threshold is not None:
                df = df[df['similarity'] >= threshold]
            
            # Process each patent_id_1
            for patent_id, group in df.groupby('patent_id_1'):
                # Sort by similarity in descending order
                sorted_group = group.sort_values('similarity', ascending=False)
                
                # Get the top_n rows
                top_rows = sorted_group.head(top_n)
                
                # Add to or update our results
                if patent_id in all_data:
                    # Combine with existing data and re-sort
                    combined = pd.concat([all_data[patent_id], top_rows])
                    combined = combined.sort_values('similarity', ascending=False).head(top_n)
                    all_data[patent_id] = combined
                else:
                    all_data[patent_id] = top_rows
        
        except Exception as e:
            logger.warning(f"Error processing file {file}: {e}")
        
        # Show progress every 30 seconds
        current_time = time.time()
        if current_time - last_update_time > 30:
            elapsed = current_time - start_time
            progress = file_idx / len(valid_files) if valid_files else 0
            logger.info(f"Progress: {file_idx+1}/{len(valid_files)} files ({progress:.1%}), "
                      f"Patents collected: {len(all_data):,}, "
                      f"Elapsed: {elapsed:.1f}s, "
                      f"ETA: {elapsed/progress-elapsed:.1f}s" if progress > 0 else "Calculating...")
            last_update_time = current_time
    
    # Write final results
    final_output = output_file + ".final" if temp_files else output_file
    _write_top_n_results(all_data, final_output)
    
    # Combine all temp files if needed
    if temp_files:
        logger.info(f"Combining {len(temp_files) + 1} temporary files...")
        try:
            # Read all temporary files and the final output into dataframes
            all_dfs = []
            for temp_file in temp_files:
                try:
                    all_dfs.append(pq.read_table(temp_file).to_pandas())
                except Exception as e:
                    logger.warning(f"Error reading temp file {temp_file}: {e}")
            
            # Add the final output
            all_dfs.append(pq.read_table(final_output).to_pandas())
            
            # Combine all dataframes
            combined_df = pd.concat(all_dfs, ignore_index=True)
            
            # Process top_n again across all data
            final_data = {}
            for patent_id, group in combined_df.groupby('patent_id_1'):
                final_data[patent_id] = group.sort_values('similarity', ascending=False).head(top_n)
            
            # Write the final combined result
            _write_top_n_results(final_data, output_file)
            
            # Clean up temporary files
            for temp_file in temp_files + [final_output]:
                try:
                    os.remove(temp_file)
                except Exception as e:
                    logger.warning(f"Error removing temporary file {temp_file}: {e}")
            
        except Exception as e:
            logger.error(f"Error combining temporary files: {e}")
            logger.info(f"You can manually combine the temporary files: {', '.join(temp_files + [final_output])}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    logger.info(f"Done! Combined top {top_n} similar patents for each patent ID.")
    logger.info(f"Output file: {output_file}")
    logger.info(f"Total processing time: {total_time:.2f} seconds")
    
    # Get file size
    output_size_bytes = os.path.getsize(output_file)
    output_size_gb = output_size_bytes / (1024**3)
    logger.info(f"Output file size: {output_size_gb:.2f} GB")
    
    # Clean up checkpoint if successful
    if os.path.exists("combine_checkpoint.txt"):
        os.remove("combine_checkpoint.txt")

def _write_top_n_results(all_data, output_file):
    """Helper function to write top_n results to a parquet file."""
    if not all_data:
        logger.warning("No data to write.")
        return
    
    # Combine all results into a single DataFrame
    logger.info("Combining results...")
    result_dfs = list(all_data.values())
    combined_df = pd.concat(result_dfs, ignore_index=True)
    
    # Ensure patent IDs are strings
    combined_df['patent_id_1'] = combined_df['patent_id_1'].astype(str)
    combined_df['patent_id_2'] = combined_df['patent_id_2'].astype(str)
    
    # Get schema
    schema = pa.schema([
        ('patent_id_1', pa.string()),
        ('patent_id_2', pa.string()),
        ('similarity', pa.float32())
    ])
    
    # Create output directory if needed
    os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
    
    # Write to parquet
    logger.info(f"Writing {len(combined_df):,} rows to {output_file}...")
    table = pa.Table.from_pandas(combined_df, schema=schema)
    pq.write_table(table, output_file)
    logger.info(f"Write complete!")

def get_resume_index():
    """Check if there's a checkpoint file and return the resume index."""
    if os.path.exists("combine_checkpoint.txt"):
        with open("combine_checkpoint.txt", "r") as f:
            try:
                return int(f.read().strip())
            except ValueError:
                return 0
    return 0

def main():
    """Main function to run the script."""
    args = parse_arguments()
    
    # Use defaults if not provided
    input_dir = args.input_dir or "/scratch/midway3/dariomarino/similarity_output1"
    output_dir = os.path.dirname(args.output_file) if args.output_file else input_dir
    os.makedirs(output_dir, exist_ok=True)
    
    output_file = args.output_file or os.path.join(input_dir, "combined_similarities.parquet")
    
    # Get list of parquet files
    files = get_parquet_files(input_dir)
    
    logger.info(f"Found {len(files)} parquet files in {input_dir}")
    
    # Check if we should resume from a checkpoint
    resume_index = args.resume_from or get_resume_index()
    if resume_index > 0:
        logger.info(f"Resuming from file index {resume_index}")
    
    # Choose the combining method based on arguments
    try:
        if args.top_n:
            combine_files_with_top_n(files, output_file, args.top_n, args.threshold, args.chunk_size, resume_index)
        else:
            combine_files_basic(files, output_file, args.threshold, args.chunk_size, resume_index)
    except KeyboardInterrupt:
        logger.info("Process interrupted by user. Progress has been saved.")
    except Exception as e:
        logger.error(f"Error in main process: {e}", exc_info=True)
        logger.info("Check logs for details. You can resume from the last checkpoint.")

if __name__ == "__main__":
    main()